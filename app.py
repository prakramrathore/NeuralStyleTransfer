# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1f0E0ZzKfnYbIjbnlRPOoO_R4m6p8IvvZ
"""

import tensorflow as tf
import streamlit as st


import os

import IPython.display as display

import matplotlib.pyplot as plt
import matplotlib as mpl
mpl.rcParams['figure.figsize'] = (12, 12)
mpl.rcParams['axes.grid'] = False

import numpy as np
import PIL.Image
import time

def tensor_to_image(tensor):
  tensor = tensor*255
  tensor = np.array(tensor, dtype=np.uint8)
  if np.ndim(tensor)>3:
    assert tensor.shape[0] == 1
    tensor = tensor[0]
  return PIL.Image.fromarray(tensor)

def load_img(path_to_img):
  max_dim = 512
  img = tf.io.read_file(path_to_img)
  img = tf.image.decode_image(img, channels=3)
  img = tf.image.convert_image_dtype(img, tf.float32)

  shape = tf.cast(tf.shape(img)[:-1], tf.float32)
  long_dim = max(shape)
  scale = max_dim / long_dim

  new_shape = tf.cast(shape * scale, tf.int32)

  img = tf.image.resize(img, new_shape)
  img = img[tf.newaxis, :]
  return img

def imshow(image, title=None):
  if len(image.shape) > 3:
    # print('Accessing')
    image = tf.squeeze(image, axis=0)

  plt.imshow(image)
  if title:
    plt.title(title)

st.set_page_config(page_title="Neural Style Transfer")

st.title("Neural Style Transfer")
if st.button("Reload"):
    # Reload the entire web page when the button is clicked
    st.write("<script>location.reload();</script>", unsafe_allow_html=True)

st.divider()
st.markdown("### <span style='color: red;'>Reload the app after every run.</span>", unsafe_allow_html=True)
st.divider()

st.write("Neural Style Transfer is a technique that uses deep learning to manipulate digital images or videos in order to adopt the appearance or visual style of another image. It takes two images—a content image and a style reference image—and blends them together so the output image looks like the content image, but “painted” in the style of the style reference image. This is achieved by optimizing the output image to match the content statistics of the content image and the style statistics of the style reference image, using a convolutional network.")

st.write("Type of image should be jpg, jpeg or png")
style_path = st.file_uploader("Upload Style Image", type=["jpg", "jpeg", "png"])
content_path = st.file_uploader("Upload Content Image", type=["jpg", "jpeg", "png"])

if st.button("load Images"):
  st.image(style_path, caption="Style Image", width=500)
  st.image(content_path, caption="Content Image", width=500)

# save image to a temp folder
if style_path is not None:
    with open(os.path.join("tempDir", style_path.name), "wb") as f:
        f.write(style_path.getbuffer())
    style_path = os.path.join("tempDir", style_path.name)

if content_path is not None:
    with open(os.path.join("tempDir", content_path.name), "wb") as f:
        f.write(content_path.getbuffer())
    content_path = os.path.join("tempDir", content_path.name)


# click on load image button
if style_path is not None:
    style_image = load_img(style_path)

if content_path is not None:
    content_image = load_img(content_path)

# st.write("Recommended values for number of epochs and steps per epoch are 2 and 10 respectively.")
st.markdown("### <span style='color: blue;'>Recommended values for number of epochs and steps per epoch are 2 and 10 respectively.</span>", unsafe_allow_html=True)

num_epochs = st.slider("Number of Epochs", 1, 20, 2)
steps = st.slider("Steps per Epoch", 1, 20, 10)


layers = ['block1_conv1', 'block1_conv2', 'block1_pool', 'block2_conv1', 'block2_conv2', 'block2_pool', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block3_conv4', 'block3_pool', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block4_conv4', 'block4_pool', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'block5_conv4', 'block5_pool']

st.write("Layers available in VGG19")
st.markdown("### <span style='color: green;'>Layers available in VGG19</span>", unsafe_allow_html=True)


for layer in layers:
  st.write(f"- {layer}")


# take input from user the index of layers for content and style layers


st.markdown("### <span style='color: green;'>Select the layers for content and style layers</span>", unsafe_allow_html=True)
st.markdown("### <span style='color: green;'>Suggestions:</span>", unsafe_allow_html=True)
st.markdown("### <span style='color: green;'>block1_conv1, block2_conv1, block3_conv1, block4_conv1, block5_conv1 for Style Layer</span>", unsafe_allow_html=True)
st.markdown("### <span style='color: green;'>block5_conv2 for Content Layer</span>", unsafe_allow_html=True)

content_layers = st.multiselect("Select Content Layers", layers)
style_layers = st.multiselect("Select Style Layers", layers)

num_content_layers = len(content_layers)
num_style_layers = len(style_layers)


if st.button("Run"):
    # Run your neural style transfer algorithm here
    # Use the uploaded style and content images, and the values for num_epochs and steps_per_epoch
    # Display the output of the algorithm
  

  def vgg_layers(layer_names):
    """ Creates a VGG model that returns a list of intermediate output values."""
    # Load our model. Load pretrained VGG, trained on ImageNet data
    vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')
    vgg.trainable = False

    outputs = [vgg.get_layer(name).output for name in layer_names]

    model = tf.keras.Model([vgg.input], outputs)
    return model

  style_extractor = vgg_layers(style_layers)
  style_outputs = style_extractor(style_image*255)

  def gram_matrix(input_tensor):
    result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)
    input_shape = tf.shape(input_tensor)
    num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)
    return result/(num_locations)

  class StyleContentModel(tf.keras.models.Model):
    def __init__(self, style_layers, content_layers):
      super(StyleContentModel, self).__init__()
      self.vgg = vgg_layers(style_layers + content_layers)
      self.style_layers = style_layers
      self.content_layers = content_layers
      self.num_style_layers = len(style_layers)
      self.vgg.trainable = False

    def call(self, inputs):
      "Expects float input in [0,1]"
      inputs = inputs*255.0
      preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs)
      outputs = self.vgg(preprocessed_input)
      style_outputs, content_outputs = (outputs[:self.num_style_layers],
                                        outputs[self.num_style_layers:])

      style_outputs = [gram_matrix(style_output)
                      for style_output in style_outputs]

      content_dict = {content_name: value
                      for content_name, value
                      in zip(self.content_layers, content_outputs)}

      style_dict = {style_name: value
                    for style_name, value
                    in zip(self.style_layers, style_outputs)}

      return {'content': content_dict, 'style': style_dict}

  extractor = StyleContentModel(style_layers, content_layers)

  style_targets = extractor(style_image)['style']
  content_targets = extractor(content_image)['content']

  image = tf.Variable(content_image)

  def clip_0_1(image):
    return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)

  opt = tf.keras.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)

  style_weight=1e-2
  content_weight=1e4

  def style_content_loss(outputs):
      style_outputs = outputs['style']
      content_outputs = outputs['content']
      style_loss = tf.add_n([tf.reduce_mean((style_outputs[name]-style_targets[name])**2) 
                            for name in style_outputs.keys()])
      style_loss *= style_weight / num_style_layers

      content_loss = tf.add_n([tf.reduce_mean((content_outputs[name]-content_targets[name])**2) 
                              for name in content_outputs.keys()])
      content_loss *= content_weight / num_content_layers
      loss = style_loss + content_loss
      # return (style_loss, content_loss)
      return (loss, style_loss, content_loss)

  @tf.function()
  def train_step(image):
      with tf.GradientTape() as tape:
        outputs = extractor(image)
        loss = style_content_loss(outputs)
        print(f"Style Loss: {loss[1]} , Content Loss: {loss[2]}")
      grad = tape.gradient(loss[0], image)
      opt.apply_gradients([(grad, image)])
      image.assign(clip_0_1(image))

  start = time.time()

  epochs = num_epochs
  steps_per_epoch = steps

  step = 0
  total_steps = epochs * steps_per_epoch

  progress_bar = st.progress(0)
  time_placeholder = st.empty()
  for n in range(epochs):
    for m in range(steps_per_epoch):
      step += 1
      train_step(image)
      progress = (step) / total_steps
      progress_bar.progress(progress)
      elapsed_time = time.time() - start
      time_placeholder.write(f"Time: {elapsed_time:.2f} seconds")

  end = time.time()
  st.write("Total time: {:.1f}".format(end-start))

  output = tensor_to_image(image)
  output.save('output.png')


st.write("Output Image")
st.image('output.png', width=500)
